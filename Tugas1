{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0/0pV1WNjmtQW0s+YyGKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b4mbanh/Tugas-Social-media-analyst/blob/main/Tugas1\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqittH4AmqyH",
        "outputId": "68828299-e68f-4c19-c6ed-c6dcc6f8639e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai scraping 10 halaman dari Detik News...\n",
            "\n",
            "Scraping Halaman Indeks #1: https://news.detik.com/indeks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Memproses Artikel Halaman 1: 100%|██████████| 20/20 [00:31<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping Halaman Indeks #2: https://news.detik.com/indeks?page=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Memproses Artikel Halaman 2: 100%|██████████| 20/20 [00:32<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping Halaman Indeks #3: https://news.detik.com/indeks?page=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Memproses Artikel Halaman 3: 100%|██████████| 20/20 [00:32<00:00,  1.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping Halaman Indeks #4: https://news.detik.com/indeks?page=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Memproses Artikel Halaman 4: 100%|██████████| 20/20 [00:32<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping Halaman Indeks #5: https://news.detik.com/indeks?page=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Memproses Artikel Halaman 5: 100%|██████████| 20/20 [00:34<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping Halaman Indeks #6: https://news.detik.com/indeks?page=6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Memproses Artikel Halaman 6:   5%|▌         | 1/20 [00:01<00:31,  1.64s/it]"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm # Library untuk membuat progress bar\n",
        "\n",
        "# HEADERS digunakan untuk \"menyamar\" sebagai browser asli.\n",
        "# Ini penting agar permintaan kita tidak mudah diblokir oleh server website.\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def scrape_article_detail(url):\n",
        "    \"\"\"\n",
        "    Fungsi ini bertugas mengunjungi satu halaman artikel spesifik.\n",
        "    Tujuannya untuk mengambil data yang lebih detail: isi berita, penulis, dan tanggal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Kirim permintaan HTTP ke URL artikel\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        # Jika halaman tidak ditemukan atau ada error, fungsi berhenti.\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "\n",
        "        # 2. Parse HTML halaman dengan BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 3. Cari elemen-elemen spesifik berdasarkan class CSS-nya\n",
        "        content_div = soup.find('div', class_='detail__body-text')\n",
        "        if not content_div: # Jika struktur konten tidak ada, hentikan.\n",
        "            return None\n",
        "\n",
        "        # Ambil semua paragraf <p> di dalam div konten dan gabungkan.\n",
        "        paragraphs = content_div.find_all('p')\n",
        "        full_content = ' '.join([p.text.strip() for p in paragraphs])\n",
        "\n",
        "        author_elem = soup.find('div', class_='detail__author')\n",
        "        author = author_elem.text.strip() if author_elem else 'N/A'\n",
        "\n",
        "        date_elem = soup.find('div', class_='detail__date')\n",
        "        published_date = date_elem.text.strip() if date_elem else 'N/A'\n",
        "\n",
        "        # 4. Kembalikan data detail dalam bentuk dictionary\n",
        "        return {\n",
        "            'author': author,\n",
        "            'published_date': published_date,\n",
        "            'content': full_content\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Jika terjadi error apapun (misal: koneksi putus), fungsi berhenti.\n",
        "        return None\n",
        "\n",
        "def scrape_detik_indeks(total_pages=10):\n",
        "    \"\"\"\n",
        "    Fungsi utama yang menjadi \"manajer\" proses scraping.\n",
        "    Fungsi ini akan menjelajahi halaman-halaman indeks berita.\n",
        "    \"\"\"\n",
        "    all_articles = [] # List kosong untuk menampung semua hasil scraping\n",
        "    base_url = 'https://news.detik.com'\n",
        "\n",
        "    print(f\"Memulai scraping {total_pages} halaman dari Detik News...\")\n",
        "\n",
        "    # Looping untuk setiap nomor halaman (dari 1 sampai total_pages)\n",
        "    for page in range(1, total_pages + 1):\n",
        "        # Logika URL: halaman 1 tidak pakai parameter, halaman 2 dst pakai ?page=...\n",
        "        if page == 1:\n",
        "            index_url = f\"{base_url}/indeks\"\n",
        "        else:\n",
        "            index_url = f\"{base_url}/indeks?page={page}\"\n",
        "\n",
        "        print(f\"\\nScraping Halaman Indeks #{page}: {index_url}\")\n",
        "\n",
        "        try:\n",
        "            # Kunjungi halaman indeks\n",
        "            response = requests.get(index_url, headers=HEADERS, timeout=10)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Gagal mengakses halaman indeks #{page}. Melanjutkan...\")\n",
        "                continue # Lanjut ke halaman berikutnya\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Cari semua \"bungkus\" artikel, yaitu tag <article>\n",
        "            articles_on_page = soup.find_all('article')\n",
        "\n",
        "            if not articles_on_page:\n",
        "                print(f\"Tidak ada artikel ditemukan di halaman {page}.\")\n",
        "                continue\n",
        "\n",
        "            # Looping untuk setiap artikel yang ditemukan di halaman indeks\n",
        "            # tqdm() akan otomatis membuat progress bar di terminal\n",
        "            for article_tag in tqdm(articles_on_page, desc=f\"Memproses Artikel Halaman {page}\"):\n",
        "\n",
        "                link_tag = article_tag.find('a')\n",
        "                if not link_tag or not link_tag.has_attr('href'):\n",
        "                    continue\n",
        "\n",
        "                article_url = link_tag['href']\n",
        "                if not article_url.startswith('http'):\n",
        "                    continue\n",
        "\n",
        "                # Ambil metadata awal dari halaman indeks\n",
        "                title_tag = article_tag.find('h3', class_='media__title')\n",
        "                title = title_tag.text.strip() if title_tag else 'N/A'\n",
        "\n",
        "                img_tag = article_tag.find('img')\n",
        "                image_url = img_tag['src'] if img_tag and img_tag.has_attr('src') else 'N/A'\n",
        "\n",
        "                category_tag = article_tag.find('span', class_='media__nav__item')\n",
        "                category = category_tag.text.strip() if category_tag else 'N/A'\n",
        "\n",
        "                # Panggil fungsi detail untuk mengambil isi berita, penulis, dll.\n",
        "                detail_data = scrape_article_detail(article_url)\n",
        "\n",
        "                # Jika data detail berhasil didapatkan, gabungkan semuanya\n",
        "                if detail_data:\n",
        "                    article_data = {\n",
        "                        'title': title,\n",
        "                        'category': category,\n",
        "                        'published_date': detail_data['published_date'],\n",
        "                        'author': detail_data['author'],\n",
        "                        'article_url': article_url,\n",
        "                        'image_url': image_url,\n",
        "                        'content': detail_data['content'],\n",
        "                        'source_page': page\n",
        "                    }\n",
        "                    # Masukkan data lengkap ke dalam list utama\n",
        "                    all_articles.append(article_data)\n",
        "\n",
        "                # Beri jeda 0.5 detik, ini etika scraping agar tidak membebani server\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Terjadi error tak terduga di halaman {page}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_articles\n",
        "\n",
        "# Ini adalah titik awal program dieksekusi\n",
        "if __name__ == \"__main__\":\n",
        "    # --- PENGATURAN ---\n",
        "    # Atur jumlah halaman yang ingin di-scrape. 10 halaman akan menghasilkan sekitar 150-200 baris.\n",
        "    # Ini jumlah yang ideal untuk menunjukkan kemampuan skrip tanpa menunggu terlalu lama.\n",
        "    JUMLAH_HALAMAN_SCRAPE = 10\n",
        "\n",
        "    # --- EKSEKUSI ---\n",
        "    scraped_data = scrape_detik_indeks(total_pages=JUMLAH_HALAMAN_SCRAPE)\n",
        "\n",
        "    # --- EKSPOR DATA ---\n",
        "    if scraped_data:\n",
        "        print(f\"\\nScraping Selesai! Total {len(scraped_data)} artikel berhasil dikumpulkan.\")\n",
        "\n",
        "        # Gunakan Pandas untuk mengubah list data menjadi format tabel\n",
        "        df = pd.DataFrame(scraped_data)\n",
        "\n",
        "        # Simpan ke file CSV (bisa dibuka di Excel)\n",
        "        csv_filename = 'hasil_scrape_detik.csv'\n",
        "        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
        "        print(f\"Data berhasil diekspor ke file CSV: {csv_filename}\")\n",
        "\n",
        "        # Simpan ke file JSON\n",
        "        json_filename = 'hasil_scrape_detik.json'\n",
        "        df.to_json(json_filename, orient='records', lines=True, force_ascii=False, indent=4)\n",
        "        print(f\"Data berhasil diekspor ke file JSON: {json_filename}\")\n",
        "\n",
        "        print(\"\\n--- Preview 5 Data Pertama ---\")\n",
        "        print(df.head())\n",
        "\n",
        "    else:\n",
        "        print(\"\\nTidak ada data yang berhasil di-scrape. Cek koneksi atau struktur website mungkin telah berubah.\")\n"
      ]
    }
  ]
}